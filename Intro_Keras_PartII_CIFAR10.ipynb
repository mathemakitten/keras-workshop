{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_Keras_PartII_CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mathemakitten/keras-workshop/blob/master/Intro_Keras_PartII_CIFAR10.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "q2foaUi_BkSj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro to Keras - Classifying Very Small Images\n",
        "\n",
        "The CIFAR-10 dataset consists of 80 million tiny (32x32) colour images in 10 classes, with 6000 images per class. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton, prominent researchers in the artificial intelligence field today.\n",
        "\n",
        "For more info on the CIFAR-10 dataset, please see Alex Krizhevsky's reference: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "We are going to train a deep convolutional neural network to identify whether an image is in one of 10 categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, or truck. There is also a CIFAR-100 dataset (with, as you may have guessed, 100 categories).\n",
        "\n",
        "This workshop is based on work created and shared by the Keras team at Google, and used according to terms described in The MIT License (MIT).\n",
        "\n",
        "Source: https://github.com/keras-team/keras/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "yYCwvUlZBkSl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Ensure that we have the right version of Keras due to dependencies \n",
        "!pip uninstall keras\n",
        "!pip install keras==2.1.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnYIizaABkSr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the Keras version\n",
        "import keras \n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmFFl1nLBkS2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our model will get to 75% validation accuracy in 25 epochs, and 79% after 50 epochs. (it's still underfitting then)\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "# Set hyperparameters -- you can change these! \n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XV09nIq4BkS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training data preparation\n",
        "\n",
        "We always want to split out our dataset into train and test subsets to ensure that we get an accurate view of whether or not the model is overfitting to random noise within the data. Test data will never been shown to the model during training and used only for calculating accuracy at the end. "
      ]
    },
    {
      "metadata": {
        "id": "QXcRz5efBkS9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbU3zeCWBkTE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding y-labels\n",
        "\n",
        "We use Keras's built-in `to-categorical` feature to convert an array of labeled data (from 0 to number_of_classes-1) to a one-hot vector (think truth tables)."
      ]
    },
    {
      "metadata": {
        "id": "F_-bFqDqBkTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-IUVl76BkTJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Putting together a model architecture\n",
        "\n",
        "The beauty of Keras is in its simplicity of syntax. Keras was designed to be human-readable and intuitive, as evidenced below. We are going to build a sequential model with several convolutional layers, a reLU activation function, a 2x2 pooling layer, and dropout to avoid overfitting. "
      ]
    },
    {
      "metadata": {
        "id": "MwAXRDo7BkTK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer: http://ruder.io/optimizing-gradient-descent/\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ueO7mL82BkTk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data augmentation\n",
        "\n",
        "We can teach our network to learn even more nuances by constantly rotating or changing small bits of the image in real-time to create a new training observation while in the middle of training."
      ]
    },
    {
      "metadata": {
        "id": "0S3sdJLrwGsk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3peXOXJSBkT-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save model to disk \n",
        "\n",
        "After we've fit a model as above, we can save its parameters to disk and then use it later on to make predictions, as below."
      ]
    },
    {
      "metadata": {
        "id": "Gqy36uM4BkUA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}